# -*- coding: utf-8 -*-
"""project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1eYc1ZoWE3NWiRW8oa3loGXWwRnrclMHd

Data import and pre-processing section
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import math 
import tensorflow as tf
from tensorflow import keras
from keras import layers
import seaborn as sns
from keras.callbacks import EarlyStopping
from sklearn.model_selection import train_test_split
from tensorboard.plugins.hparams import api as hp
import warnings
warnings.filterwarnings('ignore')


data = pd.read_csv("./age_gender.csv")
datadropi = data.drop('img_name', axis=1)
columns = ["age", "gender", "ethnicity"]
y = datadropi.drop("pixels", axis=1)
X = datadropi.drop(columns, axis=1)

for i in y.columns:
  plt.figure()
  sns.countplot(y[i], color='green')
  plt.show()

num_pixels = len(X['pixels'][0].split(" "))
# this is img_height x img_height image
img_height = math.sqrt(num_pixels)

for i in range(23705):
  qq = data['pixels'][i].split(" ")
  for j in range(len(qq)):
    qq[j] = float(qq[j])/255
  data['pixels'][i] = np.array(qq)
X = np.array(data['pixels'].tolist())
X = np.reshape(X,(-1,48,48,1))

"""Gender Section"""

def train_test_model(hparams): # used for age and gender
  model = tf.keras.models.Sequential([
    tf.keras.layers.InputLayer(input_shape=(48,48,1)),  # input【?,48,48,1】
    tf.keras.layers.Conv2D(32, (3, 3), activation='relu'),  # choose filter 32
    tf.keras.layers.BatchNormalization(),
    tf.keras.layers.MaxPooling2D((2, 2)),
    tf.keras.layers.Conv2D(64, (3, 3), activation='relu'), # (3*3*32+1)*64
    tf.keras.layers.MaxPooling2D((2, 2)),
    tf.keras.layers.Flatten(), # to 1 dimension
    tf.keras.layers.Dense(hparams[HP_NUM_UNITS], activation='relu'),# relu: max(0, x)
    tf.keras.layers.Dropout(hparams[HP_DROPOUT]),
    tf.keras.layers.Dense(1, activation='sigmoid'),
  ])
  model.compile(optimizer=hparams[HP_OPTIMIZER],loss='mse',metrics=['accuracy'])

  model.fit(X_train, y_train, epochs=1) # no need earltback for one time running
  loss,accuracy = model.evaluate(X_test, y_test)
  return accuracy

def final_training(result, result1, result2):
  inputs = keras.Input(shape=(48, 48, 1))# input【?,48,48,1】
  # used 32 fliter first
  gh = layers.Conv2D(32, (3, 3), activation='relu')(inputs) 
  gh = layers.BatchNormalization()(gh)
  gh = layers.MaxPooling2D((2, 2))(gh)
  gh = layers.Conv2D(64, (3, 3), activation='relu')(gh) # (3*3*32+1)*128
  gh = layers.MaxPooling2D((2, 2))(gh)
  gh = layers.Flatten()(gh)
  gh = layers.Dense(result, activation='relu')(gh) # use hypermeter which is the best result
  gh = layers.Dropout(rate=result1)(gh)
  gh = layers.Dense(1, activation='sigmoid')(gh)

  model = keras.Model(inputs=inputs, outputs=gh)   

  model.compile(optimizer=result2,loss='mse',metrics=['accuracy'])

  callbacks = EarlyStopping(monitor='accuracy', patience=3, verbose=2, mode='max')

  model.summary() #CNN Network layer summary 

  history = model.fit(X_train, y_train, epochs=10, validation_split=0.1, batch_size=256, callbacks=callbacks)

  plt.plot(history.history['accuracy'])
  plt.plot(history.history['val_accuracy'])
  plt.title('Model accuracy')
  plt.ylabel('Accuracy')
  plt.xlabel('Epoch')
  plt.legend(['Train', 'Test'], loc='upper left')
  plt.show()

  loss, acc = model.evaluate(X_test,y_test,verbose=0)
  print('Test loss: {}'.format(loss))
  print('Test Accuracy: {}'.format(acc))

# gender
y = data['gender']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=80)  # ensure future get same result


HP_NUM_UNITS = hp.HParam('num_units', hp.Discrete([32, 64]))
HP_DROPOUT = hp.HParam('dropout', hp.RealInterval(0.1,0.5))
HP_OPTIMIZER = hp.HParam('optimizer', hp.Discrete(['adam', 'sgd']))

METRIC_ACCURACY = 'accuracy'

hp.hparams_config(
  hparams=[HP_NUM_UNITS, HP_DROPOUT, HP_OPTIMIZER],
  metrics=[hp.Metric(METRIC_ACCURACY, display_name='Accuracy')],
)

session = 1
result= 0
result1 = 0
result2 = ''
final_acc = 0;

# follow guides from ---- Hyperparameter Tuning with the HParams Dashboard
for num_units in HP_NUM_UNITS.domain.values:
  for dropout_rate in (HP_DROPOUT.domain.min_value, HP_DROPOUT.domain.max_value):
    for optimizer in HP_OPTIMIZER.domain.values:  
      hparams = {
          HP_NUM_UNITS: num_units,
          HP_DROPOUT: dropout_rate,
          HP_OPTIMIZER: optimizer,
      }
      print('Round: %d' % session)
      print({h.name: hparams[h] for h in hparams})
      temp_acc = train_test_model(hparams)
      if temp_acc> final_acc:
        final_acc = temp_acc 
        result = num_units
        result1 = dropout_rate
        result2 = optimizer
      print(result,result1,result2)
      session += 1

# based on hyperparameter best result continue
final_training(result, result1, result2)

"""Age section

"""

def train_test_age(hparams): # used for age 
  model = tf.keras.models.Sequential([
    tf.keras.layers.InputLayer(input_shape=(48,48,1)),
    tf.keras.layers.Conv2D(32, (3, 3), activation='relu'),   # input【?,48,48,1】
    tf.keras.layers.BatchNormalization(),
    tf.keras.layers.MaxPooling2D((2, 2)),
    tf.keras.layers.Conv2D(64, (3, 3), activation='relu'), # (3*3*32+1)*128
    tf.keras.layers.MaxPooling2D((2, 2)),
    tf.keras.layers.Conv2D(128, (3, 3), activation='relu'), # (3*3*32+1)*128
    tf.keras.layers.MaxPooling2D((2, 2)),
    tf.keras.layers.Flatten(),
    tf.keras.layers.Dense(hparams[HP_NUM_UNITS], activation='relu'),
    tf.keras.layers.Dropout(hparams[HP_DROPOUT]),
    tf.keras.layers.Dense(1, activation='relu'),
  ])
  model.compile(optimizer=hparams[HP_OPTIMIZER],loss='mean_squared_error',metrics=['mae'])

  model.fit(X_train, y_train, epochs=1)
  loss,accuracy = model.evaluate(X_test, y_test)
  return accuracy

# age
y = data['age']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=50)  # ensure future get same result

HP_NUM_UNITS = hp.HParam('num_units', hp.Discrete([64,128]))
HP_DROPOUT = hp.HParam('dropout', hp.RealInterval(0.1,0.5))
HP_OPTIMIZER = hp.HParam('optimizer', hp.Discrete(['adam']))

METRIC_ACCURACY = 'accuracy'

hp.hparams_config(
  hparams=[HP_NUM_UNITS, HP_DROPOUT, HP_OPTIMIZER],
  metrics=[hp.Metric(METRIC_ACCURACY, display_name='Accuracy')],
)

session = 1
result= 0
result1 = 0
result2 = ''
final_acc = 0;

for num_units in HP_NUM_UNITS.domain.values:
  for dropout_rate in (HP_DROPOUT.domain.min_value, HP_DROPOUT.domain.max_value):
    for optimizer in HP_OPTIMIZER.domain.values:  
      hparams = {
          HP_NUM_UNITS: num_units,
          HP_DROPOUT: dropout_rate,
          HP_OPTIMIZER: optimizer,
      }
      print('Round: %d' % session)
      print({h.name: hparams[h] for h in hparams})
      temp_acc = train_test_age(hparams)
      if temp_acc> final_acc:
        final_acc = temp_acc 
        result = num_units
        result1 = dropout_rate
        result2 = optimizer
      print(result,result1,result2)
      session += 1


X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=25)  # ensure future get same result

inputs = keras.Input(shape=(48, 48, 1))

gh = layers.Conv2D(32, (3, 3), activation='relu')(inputs) # input【?,48,48,1】
gh = layers.BatchNormalization()(gh)
gh = layers.MaxPooling2D((2, 2))(gh)
gh = layers.Conv2D(64, (3, 3), activation='relu')(gh) # (3*3*32+1)*128
gh = layers.MaxPooling2D((2, 2))(gh)
gh = layers.Conv2D(128, (3, 3), activation='relu')(gh)
gh = layers.MaxPooling2D((2, 2))(gh)
gh = layers.Flatten()(gh)
gh = layers.Dense(result, activation='relu')(gh)
gh = layers.Dropout(rate=result1)(gh)
gh = layers.Dense(1, activation='relu')(gh)

model = keras.Model(inputs=inputs, outputs=gh)

model.compile(optimizer= result2, loss='mean_squared_error',metrics=['mae'])

callbacks = EarlyStopping(monitor='val_loss', patience=2, verbose=2, mode='min')

model.summary()

history = model.fit(X_train, y_train, epochs=10, validation_split=0.1, batch_size=64, callbacks=callbacks)


plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('Model loss')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.legend(['Train', 'Test'], loc='upper left')
plt.show()

loss, acc = model.evaluate(X_test,y_test,verbose=0)
print('Test loss: {}'.format(loss))

"""Grouping age solution"""

data = pd.read_csv("./age_gender.csv")
datadropi = data.drop('img_name', axis=1)
columns = ["age", "gender", "ethnicity"]
y = datadropi.drop("pixels", axis=1)
X = datadropi.drop(columns, axis=1)

num_pixels = len(X['pixels'][0].split(" "))
# this is img_height x img_height image
img_height = math.sqrt(num_pixels)

for i in range(23705):
  qq = data['pixels'][i].split(" ")
  yy = data['gender'][i]
  gg = data['ethnicity'][i]
  aa = data['age'][i].item()
  for j in range(len(qq)):
    qq[j] = float(qq[j])/255
  if ( 0<=aa and aa<=10 ):
    data['age'][i] = 1
  elif ( 4<=aa and aa<=12 ):
    data['age'][i] = 2
  elif ( 13<=aa and aa<=20 ):
    data['age'][i] = 3
  elif ( 21<=aa and aa<=25 ):
    data['age'][i] = 4
  elif ( 26<=aa and aa<=30 ):
    data['age'][i] = 5  
  elif ( 31<=aa and aa<=35 ):
    data['age'][i] = 6  
  elif ( 36<=aa and aa<=40 ):
    data['age'][i] = 7  
  elif ( 41<=aa and aa<=50 ):
    data['age'][i] = 8 
  elif ( 51<=aa and aa<=60 ):
    data['age'][i] = 9
  elif (60<=aa):
    data['age'][i] = 10
  data['pixels'][i] = np.array(qq)
X = np.array(data['pixels'].tolist())
X = np.reshape(X,(-1,48,48,1))

y = data['age']

plt.figure(1)
plt.hist(y)


X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=25)  # ensure future get same result

inputs = keras.Input(shape=(48, 48, 1))

gh = layers.Conv2D(32, (3, 3), activation='relu')(inputs) # input【?,48,48,1】
gh = layers.BatchNormalization()(gh)
gh = layers.MaxPooling2D((2, 2))(gh)
gh = layers.Conv2D(64, (3, 3), activation='relu')(gh) # (3*3*32+1)*128
gh = layers.MaxPooling2D((2, 2))(gh)
gh = layers.Conv2D(128, (3, 3), activation='relu')(gh)
gh = layers.MaxPooling2D((2, 2))(gh)
gh = layers.Flatten()(gh)
gh = layers.Dense(32, activation='relu')(gh)
gh = layers.Dropout(rate=0.5)(gh)
gh = layers.Dense(1, activation='relu')(gh)

model = keras.Model(inputs=inputs, outputs=gh)

model.compile(optimizer='adam', loss='mean_squared_error',metrics=['mae'])

callbacks = EarlyStopping(monitor='val_loss', patience=2, verbose=2, mode='min')

model.summary()

history = model.fit(X_train, y_train, epochs=10, validation_split=0.1, batch_size=64, callbacks=callbacks)


plt.figure(2)
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('Model loss')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.legend(['Train', 'Test'], loc='upper left')
plt.show()


loss, acc = model.evaluate(X_test,y_test,verbose=0)
print('Test loss: {}'.format(loss))